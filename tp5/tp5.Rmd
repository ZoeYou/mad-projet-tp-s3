---
title: "PCAKernel"
author: "You ZUO"
date: "11/25/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(kernlab)
library(FactoMineR)
library(tidyverse)
data(spam)
spam %>% PCA(.,quali.sup = 58) %>% plot(habillage = ncol(spam), choix = "ind")

res <- prcomp(spam[, -58], scale. = TRUE)
plot(res$x[,1:2], col = as.numeric(spam$type))
```

```{r}
set.seed(1)
train <- sample(1:nrow(spam), 400)

X <- spam[train, -58]  # training data

res.kpca <- kpca(~., data = X, kernel = "vanilladot", kpar = list(), features = 57)
plot(rotated(res.kpca)[,1:2], col = as.integer(spam[train,58]))
eig(res.kpca)
pcv(res.kpca)

emb <- predict(res.kpca,spam[-train,-ncol(spam)])
points(emb,col=as.integer(spam[-train,ncol(spam)]))

# calculer le pourcentage de la premiere principale composante
cumsum(eig(res.kpca))/sum(eig(res.kpca))
```
```{r}
res.kpca <- kpca(~., data = X, kernel = "polydot", kpar = list(degree = 2), features = 2)
plot(rotated(res.kpca), col = as.integer(spam[train,58]))

res.kpca <- kpca(~., data = X, kernel = "rbfdot", kpar = list(sigma = 1/500), features = 2)
plot(rotated(res.kpca), col = as.integer(spam[train,58]))
```


```{r un_example_de_KPCA}
# another example using the iris
data(iris)
test <- sample(1:150,20)

kpc <- kpca(~.,data=iris[-test,-5],kernel="rbfdot",
            kpar=list(sigma=0.2),features=2)

#print the principal component vectors
pcv(kpc)

#plot the data projection on the components
plot(rotated(kpc),col=as.integer(iris[-test,5]),
     xlab="1st Principal Component",ylab="2nd Principal Component")

#embed remaining points 
emb <- predict(kpc,iris[test,-5])
points(emb,col=as.integer(iris[test,5]))
```

```{r}
#noyau linÃ©aire
K <- as.matrix(X) %*% t(as.matrix(X)) 

#noyau gaussien 
# for (i in 1:nrow(X)) {
#   for (j in 1:i) {
#     K[i,j] <- exp(sum((X[i,]-X[j,])^2/sigma^2))
#   }
# }
```


```{r}
library(mlbench)
set.seed(111)
obj <- mlbench.spirals(n = 100,cycles = 1,sd = 0.025)
my.data <- data.frame(4 * obj$x)
names(my.data)<-c("X1","X2")
plot(my.data)
my.data<-as.matrix(my.data)

par(mfrow = c(2,1))
plot(my.data, col = c('orange', 'blue')[obj$classes], main = "True Classes")
plot(my.data, col = c('orange', 'blue')[kmeans(my.data,2)$cluster], main = "Kmeans")

```

```{r cal_K}
# method 1
library(KRLS)
K <- gausskernel(X = my.data, sigma = 1)

# my method 
sigma <- 1
my.K <- matrix(NA, nrow(my.data), nrow(my.data))
for (i in 1:nrow(my.data)) {
  for (j in 1:nrow(my.data)) {
    my.K[i,j] <- exp(-((my.data[i,]-my.data[j,])%*%(my.data[i,]-my.data[j,]))/sigma)
  }
}

# prof
K <- exp(-as.matrix(dist(my.data))^2/sigma)
image(K)
classes <- obj$classes
image(K[order(classes), order(classes)])
```

dist() calculate the $||x_i-x_j||^2$ of matrix

```{r cal_A}
A <- (K > 0.5) * K
diag(A) <- 0

D <- diag(colSums(A))
L <- D - A

val.p <- eigen(L)$values
vec.p <- eigen(L)$vectors

plot(vec.p[,100])
plot(vec.p[order(classes),100])
```

```{r}
spectral.classes <- kmeans(vec.p[,100],2)$cluster
plot(my.data, col = spectral.classes)
```

```{r}
A <- K
diag(A) <- 0

D <- diag(colSums(A))
L <- D - A

val.p <- eigen(L)$values
vec.p <- eigen(L)$vectors

plot(vec.p[,100])
plot(vec.p[order(classes),100])

spectral.classes <- kmeans(vec.p[,100],2)$cluster
plot(my.data, col = spectral.classes)
```

```{r}
library(kernlab)
sc <- specc(my.data, centers = 2)
plot(my.data, col=sc)
```

```{r kpca_kmeans}
res.kpca <- kpca(~., data = as.data.frame(my.data), kernel = "rbfdot", kpar = list(sigma=2), features = 3)
eig(res.kpca)
pcv(res.kpca)

pairs(pcv(res.kpca), col=obj$classes)
```

